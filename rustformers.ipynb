{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.indexes import  VectorstoreIndexCreator\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.llms import GPT4All\n",
    "from langchain import ConversationChain, PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "video_links = [\"9lVj_DZm36c\", \"ZUN3AFNiEgc\", \"8KtDLu4a-EM\"]\n",
    "\n",
    "if os.path.exists('transcripts'):\n",
    "    print('Directory already exists')\n",
    "else:\n",
    "    os.mkdir('transcripts')\n",
    "for video_id in video_links:\n",
    "    dir = os.path.join('transcripts', video_id)\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "    with open(dir+'.txt', 'w') as f:\n",
    "     for line in transcript:\n",
    "            f.write(f\"{line['text']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path='./', glob = \"**/*.txt\", loader_cls=TextLoader,\n",
    "                         show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 103.46it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])\n",
    "retriever = index.vectorstore.as_retriever(search_kwargs=dict(k=5))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = GPT4All(model=\"./ggml-mpt-7b-instruct.bin\", top_p=0.15, top_k=0,  temp=0.5, repeat_penalty=1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "Do not make up answers and provide only information that you have.\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "(You do not need to use these pieces of information if not relevant)\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[ \"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=PROMPT,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory = memory\n",
    "    )\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    print(conversation_with_summary.predict(input = \"How large is california\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Processor : Intel Core i5 10th Gen processor (1035G1) with 4 Cores and 8 Threads, Turbo Boost Upto 3.8 GHz, 6 MB Cache Memory 2) Operating System - Windows 11 Home Edition 64 Bit OS 3) Display Size- 15 inch FHD Anti Glare IPS display with 1920 x 1080 pixels resolution 4) Graphics Card : Intel Integrated GPU 5) RAM & Storage: 8 GB DDR4 2400 MHz SDRAM, 1 TB HDD 7200 RPM, 2 Slots for M.2 SSD 6) Battery - Li ion battery\n",
      "1) Processor : Intel Core i5 10th Gen processor (1035G1) with 4 Cores and 8 Threads, Turbo Boost Upto 3.8 GHz, 6 MB Cache Memory 2) Operating System - Windows 11 Home Edition 64 Bit OS 3) Display Size- 15 inch FHD Anti Glare IPS display with 1920 x 1080 pixels resolution 4) Graphics Card : Intel Integrated GPU 5) RAM & Storage: 8 GB DDR4 2400 MHz SDRAM, 1 TB HDD 7200 RPM, 2 Slots for M.2 SSD 6) Battery - Li ion battery\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    print(llm(PROMPT_FOR_GENERATION_FORMAT.format(instruction = \"what are the specifications of lenovo slim 7i 2023 edition\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1c8c772db42458315451b3a3254fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a5b361beca4e7bb22e1e4e66491b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04969b47c154401d8bbaff90be988088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repo_name = \"rustformers/mpt-7b-ggml\"\n",
    "file_name = \"mpt-7b-instruct-q5_1-ggjt.bin\"\n",
    "\n",
    "from llm_rs import AutoModel,SessionConfig,GenerationConfig,Precision\n",
    "\n",
    "session_config = SessionConfig(threads=2,batch_size=2)\n",
    "model = AutoModel.from_pretrained(repo_name, model_file=file_name, session_config=session_config,verbose=True, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "PanicException",
     "evalue": "called `Option::unwrap()` on a `None` value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[39myield\u001b[39;00m response\n\u001b[0;32m     15\u001b[0m stream \u001b[39m=\u001b[39m process_stream(instruction\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow big is india\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m, top_k\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, seed \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m stream:\n\u001b[0;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mprocess_stream\u001b[1;34m(instruction, temperature, top_p, top_k, max_new_tokens, seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m response \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m streamer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstream(prompt\u001b[39m=\u001b[39mprompt,generation_config\u001b[39m=\u001b[39mgeneration_config)\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m new_text \u001b[39min\u001b[39;00m streamer:\n\u001b[0;32m     12\u001b[0m     response \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_text\n\u001b[0;32m     13\u001b[0m     \u001b[39myield\u001b[39;00m response\n",
      "\u001b[1;31mPanicException\u001b[0m: called `Option::unwrap()` on a `None` value"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_stream(instruction, temperature, top_p, top_k, max_new_tokens, seed):\n",
    "\n",
    "    prompt=f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "Answer:\"\"\"\n",
    "    generation_config = GenerationConfig(seed=seed,temperature=temperature,top_p=top_p,top_k=top_k,max_new_tokens=max_new_tokens)\n",
    "    response = \"\"\n",
    "    streamer = model.stream(prompt=prompt,generation_config=generation_config)\n",
    "    for new_text in streamer:\n",
    "        response += new_text\n",
    "        yield response\n",
    "\n",
    "stream = process_stream(instruction= \"How big is india\", temperature= 0.5, top_p=0.15, top_k=0, max_new_tokens=512, seed = 42)\n",
    "\n",
    "for i in stream:\n",
    "    print(i)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
